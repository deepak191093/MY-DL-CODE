{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable plotting graphs in Jupyter notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical libraries\n",
    "import numpy as np   \n",
    "\n",
    "# Import Linear Regression machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# to handle data in form of rows and columns \n",
    "import pandas as pd    \n",
    "\n",
    "# importing ploting libraries\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "#importing seaborn for statistical plots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the CSV file into pandas dataframe\n",
    "mpg_df = pd.read_csv(\"car-mpg.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top few records to get a feel of the data structure\n",
    "mpg_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the car name column as it is useless for the model\n",
    "mpg_df = mpg_df.drop('car_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the numbers in categorical variables with the actual country names in the origin col\n",
    "mpg_df['origin'] = mpg_df['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable into dummy/indicator variables. As many columns will be created as distinct values\n",
    "# This is also kown as one hot coding. The column names will be America, Europe and Asia... with one hot coding\n",
    "mpg_df = pd.get_dummies(mpg_df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets analysze the distribution of the dependent (mpg) column\n",
    "mpg_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  HP column is missing the describe output. That indicates something is not right with that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df['hp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in   mpg_df.hp.str.isdigit() if x ==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the hp column contains anything other than digits \n",
    "# run the \"isdigit() check on 'hp' column of the mpg_df dataframe. Result will be True or False for every row\n",
    "# capture the result in temp dataframe and dow a frequency count using value_counts()\n",
    "# There are six records with non digit values in 'hp' column\n",
    "temp = pd.DataFrame(mpg_df.hp.str.isdigit())  # if the string is made of digits store True else False  in the hp column \n",
    "# in temp dataframe\n",
    "\n",
    "# temp[temp['hp'] == False]   # from temp take only those rows where hp has false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['hp'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On inspecting records number 32, 126 etc, we find \"?\" in the columns. Replace them with \"nan\"\n",
    "#Replace them with nan and remove the records from the data frame that have \"nan\"\n",
    "mpg_df = mpg_df.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see if we can get those records with nan\n",
    "\n",
    "mpg_df[mpg_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are various ways to handle missing values. Drop the rows, replace missing values with median values etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#of the 398 rows 6 have NAN in the hp column. We will drop those 6 rows. Not a good idea under all situations\n",
    "#note: HP is missing becauses of the non-numeric values in the column. \n",
    "#mpg_df = mpg_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of dropping the rows, lets replace the missing values with median value. \n",
    "mpg_df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the missing values in 'hp' with median value of 'hp' :Note, we do not need to specify the column names\n",
    "# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)\n",
    "mpg_df = mpg_df.fillna(mpg_df.median())\n",
    "\n",
    "# mpg_df = mpg_df.apply(lambda x: x.fillna(x.median()),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df['hp'] = mpg_df['hp'].astype('float64')  # converting the hp column from object / string type to float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us do a correlation analysis among the different dimensions and also each dimension with the dependent dimension\n",
    "# This is done using scatter matrix function which creates a dashboard reflecting useful information about the dimensions\n",
    "# The result can be stored as a .png file and opened in say, paint to get a larger view \n",
    "\n",
    "mpg_df_attr = mpg_df.iloc[:, 0:11]\n",
    "\n",
    "\n",
    "#axes = pd.plotting.scatter_matrix(mpg_df_attr)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('d:\\greatlakes\\mpg_pairpanel.png')\n",
    "\n",
    "sns.pairplot(mpg_df_attr, diag_kind='kde')   # to plot density curve instead of histogram\n",
    "\n",
    "#sns.pairplot(mpg_df_attr)  # to plot histogram, the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(mpg_df_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data distribution across various dimensions except 'Acc' do not look normal\n",
    "#Close observation between 'mpg' and other attributes indicate the relationship is not really linear\n",
    "#relation between 'mpg' and 'hp' show hetroscedacity... which will impact model accuracy\n",
    "#How about 'mpg' vs 'yr' surprising to see a positive relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all the predictor variables into X dataframe. Since 'mpg' is dependent variable drop it\n",
    "X = mpg_df.drop('mpg', axis=1)\n",
    "\n",
    "# Copy the 'mpg' column alone into the y dataframe. This is the dependent variable\n",
    "y = mpg_df[['mpg']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us break the X and y dataframes into training set and test set. For this we will use\n",
    "#Sklearn package's data splitting function which is based on random function\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 75:25 ratio\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the LinearRegression function and find the bestfit model on training data\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore the coefficients for each of the independent attributes\n",
    "\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the intercept for the model\n",
    "\n",
    "intercept = regression_model.intercept_[0]\n",
    "\n",
    "print(\"The intercept for our model is {}\".format(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can write our linear model as:\n",
    "# Y=−21.11–0.35×X1+0.03×X2–0.02×X3–0.01×X4+0.12×X5+0.85×X6–1.90×X7+0.74×X8+1.16×X9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score - R2 or coeff of determinant\n",
    "# R^2=1–RSS / TSS\n",
    "\n",
    "regression_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the model explains 85% of the variability in Y using X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Iteration -2 \n",
    "\n",
    "#Since on many dimensions, the relationship is not really linear, let us try polynomial models (quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_ = poly.fit_transform(X_train)\n",
    "\n",
    "X_test_ = poly.fit_transform(X_test)\n",
    "\n",
    "poly_clf = linear_model.LinearRegression()\n",
    "\n",
    "poly_clf.fit(X_train_, y_train)\n",
    "\n",
    "y_pred = poly_clf.predict(X_test_)\n",
    "\n",
    "#print(y_pred)\n",
    "\n",
    "print(poly_clf.score(X_test_, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_train_.shape)\n",
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even with polynomial function, we are not getting better results. What Next?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
